# -*- coding: utf-8 -*-
"""LangChainQA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-1ndgyJHtGNdgGkD5T2ECLWahOgyOpiX
"""

!pip install chromadb langchain openai tiktoken

!pip install pypdf

from langchain.document_loaders import PyPDFLoader
loaders = PyPDFLoader("Cs229.pdf")

#Load the document by calling loader.load()
pages = loaders.load()

print(len(pages))
print(pages[0].page_content[0:500])

docs = []

loaders = [PyPDFLoader("Cs229.pdf")]

for loader in loaders:
    docs.extend(loader.load())

# Define the Text Splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)

#Create a split of the document using the text splitter
splits = text_splitter.split_documents(docs)

import os
os.environ['OPENAI_API_KEY'] = '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'

from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings

embedding = OpenAIEmbeddings()

persist_directory = './chroma_db'

# Create the vector store
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)

print(vectordb._collection.count())

question = "what did it say about Generalized Linear Models ?"

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)

from langchain.prompts import PromptTemplate

# Build prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# Initilaize chain
# Set return_source_documents to True to get the source document
# Set chain_type to prompt template defines
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

result = qa_chain({"query": question})

# Check the result of the query
result["result"]

# Check the source document from where we
result["source_documents"][0]

qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type="map_reduce"
)
result = qa_chain_mr({"query": question})
result["result"]

qa_chain_r = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type="refine"
)
result = qa_chain_r({"query": question})
result["result"]